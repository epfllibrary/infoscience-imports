{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13de484b-d51e-43dd-9c26-c4c42b02c215",
   "metadata": {},
   "source": [
    "# Import WOS to Infoscience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc10579-1ee5-4ba2-933e-43be357c3aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wos_client import WosClient\n",
    "from csv import DictWriter\n",
    "import os, base64\n",
    "import logging\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from dspace.client import DSpaceClient\n",
    "import config\n",
    "import utils\n",
    "from dotenv import load_dotenv\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "#pio.renderers.default = \"vscode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a83b5c7-6679-434f-8136-817bb6510de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Workflow metadata #####################\n",
    "current_date = config.CURRENT_DATE\n",
    "folder_path = \"harvested-data\"\n",
    "path = os.path.join(folder_path, str(current_date).replace(\"-\", \"_\"))\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "doctypes = [key for key in utils.get_doctype_mapping().keys()]\n",
    "\n",
    "databaseId = \"WOS\"\n",
    "epfl_query = \"OG=(Ecole Polytechnique Federale de Lausanne) AND DT=article\"\n",
    "# epfl_query = \"AI=(GCM-6397-2022) AND DT=article\"\n",
    "# createdTimeSpan = \"2023-10-01+2024-04-01\"\n",
    "\n",
    "### global vars ###############################\n",
    "total = 0\n",
    "recs = []\n",
    "ids_to_load = []\n",
    "\n",
    "### env vars #################################\n",
    "load_dotenv()\n",
    "\n",
    "### ipywidgets config #########################\n",
    "style = {'description_width': 'initial'}\n",
    "spinner_output = widgets.Output()\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(path, \"execute.log\"),\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    encoding=\"utf-8\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b50852-f511-476e-9d22-ab22a010b0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common functions ###############################\n",
    "def create_download_link(filename, title = \"Cliquer ici pour télécharger le fichier : \"):  \n",
    "    data = open(filename, \"rb\").read()\n",
    "    b64 = base64.b64encode(data)\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title+f' {filename}',filename=filename)\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef216f-9204-43ce-a868-07813cdfb806",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b841ae-590e-4ae3-b484-1899f17d32b4",
   "metadata": {},
   "source": [
    "## Etape 1 : Récupération des WOS IDs des publications EPFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5d94a0-0c68-468c-92f8-1ab3f32941ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wos_harvesting(start_date, end_date):\n",
    "    createdTimeSpan = f\"{start_date}+{end_date}\"\n",
    "    global total\n",
    "    total = WosClient.count_results(\n",
    "      databaseId=databaseId, usrQuery=epfl_query, createdTimeSpan=createdTimeSpan\n",
    "    )\n",
    "    print(f\"- Nombre de publications trouvées dans le WOS: {total}\")\n",
    "    count = 50\n",
    "    global recs\n",
    "    for i in range(1, int(total), int(count)):\n",
    "        print(\n",
    "          f\"Harvest publications {str(i)} to {str(int(i) + int(count))} on a total of {str(total)} publications\"\n",
    "        )\n",
    "        logging.info(\n",
    "          f\"Harvest publications {str(i)} to {str(int(i) + int(count))} on a total of {str(total)} publications\"\n",
    "        )\n",
    "        h_recs = WosClient.get_wos_digest(\n",
    "          databaseId=databaseId,\n",
    "          usrQuery=epfl_query,\n",
    "          count=count,\n",
    "          firstRecord=i,\n",
    "          createdTimeSpan=createdTimeSpan,\n",
    "        )\n",
    "        filtered_recs = [item for item in h_recs if item.get(\"doctype\") in doctypes]\n",
    "        recs.extend(filtered_recs)\n",
    "    df_wos_harvest = (pd.DataFrame(recs)\n",
    "                      .dropna(how='all')\n",
    "                      .to_csv(\n",
    "                       os.path.join(path, \"AllWosHarvestedPublications.csv\"), index=False, encoding=\"utf-8\")\n",
    "                     )\n",
    "    print(f\"- Fichier csv archivé dans {os.path.join(path,'AllWosHarvestedPublications.csv')}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8697952-1eb3-44c9-88f6-4c7e251cdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = widgets.DatePicker(\n",
    "    description='Date de publication postérieure au : ',\n",
    "    value=pd.to_datetime('2024-07-22'),\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    layout = Layout(width='50%', height='30px', display='flex')\n",
    ")\n",
    "\n",
    "end_date = widgets.DatePicker(\n",
    "    description=\"Date de publication antérieure au : \",\n",
    "    value=pd.to_datetime(\"2024-08-01\"),\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    layout=Layout(width=\"50%\", height=\"30px\", display=\"flex\"),\n",
    ")\n",
    "\n",
    "harvest_button = widgets.Button(description=\"Harvest the WOS\")\n",
    "harvest_output = widgets.Output()\n",
    "\n",
    "display(start_date, end_date,harvest_button, harvest_output, spinner_output)\n",
    "\n",
    "def harvest_button_clicked(b):\n",
    "    with spinner_output:\n",
    "        display(widgets.HTML(\"<p>Chargement en cours...</p>\"))\n",
    "    with harvest_output:\n",
    "        data_harvested = wos_harvesting(start_date.value, end_date.value)\n",
    "        if data_harvested:\n",
    "            display(create_download_link(os.path.join(path,\"AllWosHarvestedPublications.csv\")))\n",
    "        spinner_output.clear_output()\n",
    "\n",
    "harvest_button.on_click(harvest_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3abef8-7809-413f-a630-626208d34946",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fb14e-7323-4393-822a-2211078abdc0",
   "metadata": {},
   "source": [
    "## Etape 2 : Dédoublonnage sur les publications déjà présentes dans Infoscience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1791f-84ce-43d4-9caa-edf1f38a941f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wos_ifs_dedup(recs):\n",
    "    # Instantiate DSpace client\n",
    "    d = DSpaceClient()\n",
    "\n",
    "    # Authenticate against the DSpace client\n",
    "    authenticated = d.authenticate()\n",
    "\n",
    "    global ids_to_load\n",
    "    unloaded_duplicated_publications = []\n",
    "    cache = {}  # Cache to store API responses and avoid repeat calls\n",
    "\n",
    "    for x in recs:\n",
    "        # Clean the title once and use it for deduplication\n",
    "        cleaned_title = utils.clean_title(x[\"title\"])\n",
    "        pubyear = x[\"pubyear\"]\n",
    "        if isinstance(pubyear, str) and pubyear.isdigit():\n",
    "            pubyear = int(pubyear)\n",
    "        elif not isinstance(pubyear, int):\n",
    "            raise ValueError(\"pubyear doit être numérique\")\n",
    "        previous_year = pubyear - 1\n",
    "        next_year = pubyear + 1\n",
    "\n",
    "        # Build queries for each matching rule\n",
    "        wos_query = f\"(itemidentifier:{str(x['wos_id'][4:]).strip()})\"\n",
    "        title_query = f\"(title:({cleaned_title}) AND (dateIssued:{pubyear} OR dateIssued:{previous_year} OR dateIssued:{next_year}))\"\n",
    "        doi_query = f\"(itemidentifier:{str(x['doi']).strip()})\" if \"doi\" in x else None\n",
    "\n",
    "        # Check each identifier, stopping if a duplicate is found\n",
    "        for query in [wos_query, title_query, doi_query]:\n",
    "            if query is None:\n",
    "                continue\n",
    "\n",
    "            # Use cached results if available\n",
    "            if query in cache:\n",
    "                is_duplicate = cache[query]\n",
    "            else:\n",
    "                # Check the researchoutput configuration\n",
    "                dsos_researchoutputs = d.search_objects(\n",
    "                    query=query,\n",
    "                    page=0,\n",
    "                    size=1,\n",
    "                    dso_type=\"item\",\n",
    "                    configuration=\"researchoutputs\",\n",
    "                )\n",
    "                num_items_researchoutputs = len(dsos_researchoutputs)\n",
    "\n",
    "                # Check the supervision configuration\n",
    "                dsos_supervision = d.search_objects(\n",
    "                    query=query,\n",
    "                    page=0,\n",
    "                    size=1,\n",
    "                    configuration=\"supervision\",\n",
    "                )\n",
    "                num_items_supervision = len(dsos_supervision)\n",
    "\n",
    "                # Determine if the item is a duplicate in either configuration\n",
    "                is_duplicate = (num_items_researchoutputs > 0) or (\n",
    "                    num_items_supervision > 0\n",
    "                )\n",
    "\n",
    "                # Cache the result\n",
    "                cache[query] = is_duplicate\n",
    "\n",
    "            if is_duplicate:\n",
    "                unloaded_duplicated_publications.append(x)\n",
    "                break\n",
    "        else:\n",
    "            # No duplicates found after all checks\n",
    "            ids_to_load.append(x[\"wos_id\"])\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    df_unload = (\n",
    "        pd.DataFrame(unloaded_duplicated_publications)\n",
    "        .dropna(how=\"all\")\n",
    "        .to_csv(\n",
    "            os.path.join(path, \"UnloadedDuplicatedPublications.csv\"),\n",
    "            index=False,\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "    )\n",
    "    df_unload = pd.read_csv(\n",
    "        os.path.join(path, \"UnloadedDuplicatedPublications.csv\"),\n",
    "        sep=\",\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    print(f\"- Nombre de publications déjà présentes dans Infoscience : {str(df_unload.shape[0])}\")\n",
    "    print(f\"- Fichier csv des publications doublons archivé dans {os.path.join(path, 'UnloadedDuplicatedPublications.csv')}\")\n",
    "    print(f\"- Nombre de publications à importer : {str(int(total) - int(df_unload.shape[0]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236092d4-e246-4da1-b459-a71473a27605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dedup_button = widgets.Button(description=\"Lancer le dédoublonnage\")\n",
    "dedup_output = widgets.Output()\n",
    "spinner_output = widgets.Output()\n",
    "display(dedup_button, dedup_output, spinner_output)\n",
    "\n",
    "def dedup_button_clicked(b):\n",
    "    with spinner_output:\n",
    "        display(widgets.HTML(\"<p>Chargement en cours...</p>\"))\n",
    "\n",
    "    with dedup_output:\n",
    "        duplicates = wos_ifs_dedup(recs)\n",
    "        if duplicates:\n",
    "            display(create_download_link(os.path.join(path, \"UnloadedDuplicatedPublications.csv\")))\n",
    "        spinner_output.clear_output()\n",
    "\n",
    "dedup_button.on_click(dedup_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e45f9-affa-4097-b9fc-6edb8df3821c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bff87-a05f-4fc7-ab73-333c0178a0ab",
   "metadata": {},
   "source": [
    "## Etape 3 : Moissonnage des métadonnées du WOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3753c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wos_metadata_retrieval(ids_to_load):\n",
    "    # Information log\n",
    "    logging.info(\"Etape 3 : Moissonnage des métadonnées du WOS\")\n",
    "    logging.info(\"Starting...\")\n",
    "\n",
    "    # Launch workflow\n",
    "    appended_data = []\n",
    "    appended_auth = []\n",
    "    no_record_ids = []\n",
    "    error_ids = []\n",
    "\n",
    "    for id in ids_to_load:\n",
    "        try:\n",
    "            result = WosClient.query_unique_id(id, infoscience_format=True)\n",
    "            if result is not None:\n",
    "                authors_record = {\n",
    "                    \"wos_id\": result[\"wos_id\"],\n",
    "                    \"authors\": result[\"authors\"],\n",
    "                }\n",
    "                df_authors = pd.DataFrame(authors_record)\n",
    "                record = result.copy()\n",
    "                del record[\"authors\"]\n",
    "                df = pd.json_normalize(record, max_level=3)\n",
    "                appended_data.append(df)\n",
    "                appended_auth.append(df_authors)\n",
    "            else:\n",
    "                logging.info(f\"None value for {id}\")\n",
    "                no_record_ids.append(id)\n",
    "        except Exception as e:\n",
    "            error_ids.append({\"wos_id\": id, \"error\": str(e)})\n",
    "\n",
    "    # Log processing results\n",
    "    df_no_record_ids = pd.DataFrame(data={\"wos_id\": no_record_ids}).dropna(how=\"all\")\n",
    "    df_no_record_ids.to_csv(\n",
    "        os.path.join(path, \"EmptyWosRecords.csv\"),\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    df_error_ids = pd.DataFrame(error_ids).dropna(how=\"all\")\n",
    "    df_error_ids.to_csv(\n",
    "        os.path.join(path,  \"ErrorProcessingWosRecords.csv\"),\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Concatenate and save all data\n",
    "    appended_data = pd.concat(appended_data).dropna(how=\"all\")\n",
    "    appended_auth = pd.concat(appended_auth).dropna(how=\"all\")\n",
    "    df_auth_final = pd.concat(\n",
    "        [appended_auth[\"wos_id\"], appended_auth[\"authors\"].apply(pd.Series)], axis=1\n",
    "    )\n",
    "\n",
    "    appended_data.dropna(how=\"all\").to_csv(\n",
    "        os.path.join(path, \"ResearchOutput.csv\"),\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"Fichier csv des métadonnées des publications archivé dans {os.path.join(path, 'ResearchOutput.csv')}\"\n",
    "    )\n",
    "\n",
    "    df_auth_final.dropna(how=\"all\").to_csv(\n",
    "        os.path.join(path, \"AddressesAndNames.csv\"),\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"Fichier csv des informations auteurs avec affiliations et corporate auteurs archivé dans {os.path.join(path, 'AddressesAndNames.csv')}\"\n",
    "    )\n",
    "\n",
    "    # Complete authors & labs\n",
    "    logging.info(\"Enrichissement des métadonnées auteurs & laboratoires\")\n",
    "\n",
    "    df_auth_epfl_only = df_auth_final.loc[\n",
    "        (\n",
    "            (\n",
    "                (df_auth_final[\"role\"].isin([\"author\"]))\n",
    "                & (df_auth_final[\"organizations\"].notna())\n",
    "            )\n",
    "            & (\n",
    "                df_auth_final[\"organizations\"].str.contains(\n",
    "                    \"EPF\", case=False, regex=False, na=False\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        | (\n",
    "            df_auth_final[\"organizations\"].str.contains(\n",
    "                \"Ecole Polytech Fed Lausanne\", case=False, regex=False, na=False\n",
    "            )\n",
    "        )\n",
    "        | (\n",
    "            df_auth_final[\"organizations\"].str.contains(\n",
    "                \"Ecole Polytechnique Federale de Lausanne\",\n",
    "                case=False,\n",
    "                regex=False,\n",
    "                na=False,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    authors_list = list(set(df_auth_epfl_only[\"full_name\"].values.tolist()))\n",
    "    field_names = [\"full_name\", \"author_infos\"]\n",
    "\n",
    "    with open(\n",
    "        os.path.join(path, \"epfl_authors_and_labs_metadata.csv\"),\n",
    "        \"a\",\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\",\n",
    "    ) as f_object:\n",
    "        for x in authors_list:\n",
    "            harvest_metadata = {\n",
    "                \"full_name\": x.strip(),\n",
    "                \"author_infos\": utils.enrich_author(path, x.strip()),\n",
    "            }\n",
    "            dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "            dictwriter_object.writerow(harvest_metadata)\n",
    "\n",
    "    df_auth_epfl_only = pd.read_csv(\n",
    "        os.path.join(path, \"epfl_authors_and_labs_metadata.csv\"),\n",
    "        sep=\",\",\n",
    "        encoding=\"utf-8\",\n",
    "    ).dropna(how=\"all\")\n",
    "    df_auth_epfl_only.columns = [\"full_name\", \"author_infos\"]\n",
    "    df_auth_epfl_only.to_csv(\n",
    "        os.path.join(path, \"epfl_authors_and_labs_metadata.csv\"),\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    df = pd.merge(\n",
    "        df_auth_final,\n",
    "        df_auth_epfl_only,\n",
    "        how=\"left\",\n",
    "        left_on=[\"full_name\"],\n",
    "        right_on=[\"full_name\"],\n",
    "    ).dropna(how=\"all\")\n",
    "    df.to_csv(\n",
    "        os.path.join(path, \"AddressesAndNames.csv\"),\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Adjust column names for control file\n",
    "    df_controle_epfl_authors_and_labs_metadata = pd.read_csv(\n",
    "        os.path.join(\n",
    "            path, \"controle_epfl_authors_and_labs_metadata.csv\"\n",
    "        ),\n",
    "        sep=\",\",\n",
    "        encoding=\"utf-8\",\n",
    "    ).dropna(how=\"all\")\n",
    "    df_controle_epfl_authors_and_labs_metadata.columns = [\n",
    "        \"authority_type\",\n",
    "        \"label\",\n",
    "        \"source_metadata\",\n",
    "        \"harvested_metadata\",\n",
    "    ]\n",
    "    df_controle_epfl_authors_and_labs_metadata.to_csv(\n",
    "        os.path.join(\n",
    "            path, \"controle_epfl_authors_and_labs_metadata.csv\"\n",
    "        ),\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    logging.info(\"Etape 3 terminée\")\n",
    "    logging.info(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e60ba-f2bb-405b-8361-ca862a359b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wos_retrieval_button = widgets.Button(description=\"Moissonner le WOS\")\n",
    "wos_retrieval_output = widgets.Output()\n",
    "display(wos_retrieval_button, wos_retrieval_output, spinner_output)\n",
    "\n",
    "def wos_retrieval_button_clicked(b):\n",
    "    with spinner_output:\n",
    "        display(widgets.HTML(\"<p>Chargement en cours...</p>\"))\n",
    "    with wos_retrieval_output:\n",
    "        df_auth_final = wos_metadata_retrieval(ids_to_load)\n",
    "        # complete_authors_and_labs(df_auth_final)\n",
    "        display(Markdown('''### Publications'''))\n",
    "        display(create_download_link(os.path.join(path,\"ResearchOutput.csv\")))\n",
    "        display(create_download_link(os.path.join(path,\"AddressesAndNames.csv\")))\n",
    "    spinner_output.clear_output()\n",
    "\n",
    "wos_retrieval_button.on_click(wos_retrieval_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ab1c9-9115-4008-9ec2-db1d3d725729",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f047f",
   "metadata": {},
   "source": [
    "## Etape 4 : Importation en tant que workspace items dans DSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4f876-4962-4112-97d4-8b962e008d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wos_to_dspace():\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(path, \"ResearchOutput.csv\"), sep=\",\", encoding=\"utf-8\"\n",
    "    )\n",
    "    if \"workspace_item_id\" not in df.columns:\n",
    "        df[\"workspace_item_id\"] = None\n",
    "\n",
    "    # Instantiate DSpace client\n",
    "    d = DSpaceClient()\n",
    "\n",
    "    # Authenticate against the DSpace client\n",
    "    authenticated = d.authenticate()\n",
    "    collection_id = \"8a8d3310-6535-4d3a-90b6-2a4428097b5b\"\n",
    "\n",
    "    ids_to_workspace = df[\"wos_id\"].tolist()\n",
    "    for index, wos_id in enumerate(ids_to_workspace):\n",
    "        response = d.create_workspaceitem_from_external_source(\"wos\", wos_id, collection_id)\n",
    "\n",
    "        workspace_id = response.get(\"id\")\n",
    "        units = utils.get_units_for_id(path, wos_id)\n",
    "        sponsorships = []\n",
    "        for unit in units:\n",
    "            sponsorships.append(\n",
    "                {\n",
    "                    \"value\": unit.get(\"acro\"),\n",
    "                    \"language\": None,\n",
    "                    \"authority\": f\"will be referenced::ACRONYM::{unit.get('acro')}\",\n",
    "                    \"securityLevel\": 0,\n",
    "                    \"confidence\": 400,\n",
    "                    \"place\": 0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        patch_operations = [\n",
    "            {\n",
    "                \"op\": \"add\",\n",
    "                \"path\": \"/sections/article_details/dc.language.iso\",\n",
    "                \"value\": [\n",
    "                    {\n",
    "                        \"value\": \"en\",\n",
    "                        \"language\": None,\n",
    "                        \"authority\": None,\n",
    "                        \"display\": \"English\",\n",
    "                        \"securityLevel\": 0,\n",
    "                        \"confidence\": -1,\n",
    "                        \"place\": 0,\n",
    "                        \"otherInformation\": None,\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"op\": \"add\",\n",
    "                \"path\": \"/sections/article_details/dc.description.sponsorship\",\n",
    "                \"value\": sponsorships,\n",
    "            },\n",
    "            {\n",
    "                \"op\": \"add\",\n",
    "                \"path\": \"/sections/article_details/epfl.peerreviewed\",\n",
    "                \"value\": [\n",
    "                    {\n",
    "                        \"value\": \"REVIEWED\",\n",
    "                        \"language\": None,\n",
    "                        \"authority\": None,\n",
    "                        \"display\": \"REVIEWED\",\n",
    "                        \"securityLevel\": 0,\n",
    "                        \"confidence\": -1,\n",
    "                        \"place\": 0,\n",
    "                        \"otherInformation\": None,\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"op\": \"add\",\n",
    "                \"path\": \"/sections/article_details/epfl.writtenAt\",\n",
    "                \"value\": [\n",
    "                    {\n",
    "                        \"value\": \"EPFL\",\n",
    "                        \"language\": None,\n",
    "                        \"authority\": None,\n",
    "                        \"display\": \"EPFL\",\n",
    "                        \"securityLevel\": 0,\n",
    "                        \"confidence\": -1,\n",
    "                        \"place\": 0,\n",
    "                        \"otherInformation\": None,\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"op\": \"add\", \"path\": \"/sections/license/granted\", \"value\": \"true\"},\n",
    "        ]\n",
    "        df.at[index, \"workspace_item_id\"] = workspace_id\n",
    "        try:\n",
    "            update_response = d.update_workspaceitem(workspace_id, patch_operations)\n",
    "            if update_response:\n",
    "                try:\n",
    "                    wf_response = d.create_workflowitem(workspace_id)\n",
    "                    logging.info(f\"Workflow item #{workspace_id} created\")\n",
    "                except Exception as e:\n",
    "                    logging.error(\n",
    "                        f\"An error occurred while creating workflow item: {str(e)}\"\n",
    "                    )\n",
    "            else:\n",
    "                logging.error(\n",
    "                    f\"Failed to update workspace item with ID: {workspace_id}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while updating workspace item: {str(e)}\")\n",
    "\n",
    "    df.to_csv(\n",
    "        os.path.join(path, \"ResearchOutput.csv\"), sep=\",\", encoding=\"utf-8\", index=False\n",
    "    )\n",
    "\n",
    "# create workspace and workflow items\n",
    "wos_to_dspace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
